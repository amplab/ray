# Parallelizing TRPO

In this example, we show how TRPO can be parallelized using Ray. We will be
working with John Schulman's
[modular_rl code](https://github.com/joschu/modular_rl).

For this tutorial I'll assume that you have Anaconda with Python 2.7 installed.

## Setting up the single core implementation of TRPO

First, we will run the original TRPO code.

Install these dependencies:

- [Gym](https://gym.openai.com/)
- The following Python packages:

    ```
    pip install theano
    pip install keras
    pip install tabulate
    ```

Then run
```
git clone https://github.com/joschu/modular_rl
cd modular_rl
./run_pg.py --env Pong-ram-v0 --agent modular_rl.agentzoo.TrpoAgent --video 0 --n_iter 500 --filter 0
```

**Note: On some versions of Mac OS X, this produces NaNs.**

On a m4.4xlarge EC2 instance, the first 10 iterations take 106s.


Each iteration consists of two phases. In the first phase, the rollouts are
computed (on one core). In the second phase, the objective is optimized, which
makes use of the parallel BLAS library. The code for all of this is in
`modular_rl/modular_rl/core.py`.

```python
for _ in xrange(cfg["n_iter"]):
  # Rollouts ========
  paths = get_paths(env, agent, cfg, seed_iter)
  compute_advantage(agent.baseline, paths, gamma=cfg["gamma"], lam=cfg["lam"])
  # VF Update ========
  vf_stats = agent.baseline.fit(paths)
  # Pol Update ========
  pol_stats = agent.updater(paths)
```

We will now see how this code can be parallelized.

## Parallelizing TRPO rollouts using Ray

As a first step, we will parallelize the rollouts. This is done by implementing
a function `do_rollouts_remote` similar to
[do_rollouts_serial](https://github.com/joschu/modular_rl/blob/46a6f9a0d363a7bc1c7325ff17e2eb684612a388/modular_rl/core.py#L137),
which will be called by
[get_paths](https://github.com/joschu/modular_rl/blob/46a6f9a0d363a7bc1c7325ff17e2eb684612a388/modular_rl/core.py#L102)
(called in the above code snippet).

Check out the parallel version of the TRPO code.

```
git clone https://github.com/pcmoritz/modular_rl modular_rl_ray
cd modular_rl_ray
git checkout remote
```

You can run the code using
```
./run_pg.py --env Pong-ram-v0 --agent modular_rl.agentzoo.TrpoAgent --video 0 --n_iter 500 --filter 1 --remote 1 --n_rollouts 8
```

There are few [changes](https://github.com/joschu/modular_rl/compare/master...pcmoritz:23d3ebc).
As in the [learning to play Pong example](https://github.com/amplab/ray/tree/master/examples/rl_pong),
we use reusable variables to store the gym environment and the neural network policy. These are
then used in the remote `do_rollout` function to do a remote rollout:

```python
@ray.remote([np.ndarray, int, int], [dict])
def do_rollout(policy, timestep_limit, seed):
  # Retrieve the game environment.
  env = ray.reusables.env
  # Set the environment seed.
  env.seed(seed)
  # Set the numpy seed.
  np.random.seed(seed)
  # Retrieve the neural network agent.
  agent = ray.reusables.agent
  # Set the network weights.
  agent.set_from_flat(policy)
  return rollout(env, agent, timestep_limit)
```

All that is left is to invoke the remote function and collect the paths.

```python
def do_rollouts_remote(agent, timestep_limit, n_timesteps, n_parallel, seed_iter):
  # Put the neural network weights into the object store.
  policy = ray.put(agent.get_flat())
  paths = []
  timesteps_sofar = 0
  # Run parallel rollouts until we have enough.
  while timesteps_sofar < n_timesteps:
    # Launch rollout tasks in parallel.
    rollout_ids = [do_rollout.remote(policy, timestep_limit, seed_iter.next()) for i in range(n_parallel)]
    for rollout_id in rollout_ids:
      # Retrieve the task output from the object store.
      path = ray.get(rollout_id)
      paths.append(path)
      timesteps_sofar += pathlength(path)
  return paths
```

On the same m4.4xlarge EC2 instance, the first 10 iterations now take 42s instead of
106s.

## Running TRPO on a cluster

You should follow the
[guide](https://github.com/amplab/ray/blob/master/doc/using-ray-on-a-cluster.md)
to set up a Ray cluster. For these experiments I ran


```
python ec2.py --key-pair=awskey \
              --identity-file=awskey.pem \
              --region=us-west-1 \
              --master-instance-type=c4.8xlarge \
              --instance-type=c4.4xlarge \
              --spot-price=2.50 \
              --slaves=2 \
              launch my-ray-cluster
```

inside of the ray/scripts directory.

I recommend installing Anaconda. To do this, copy the file
`ray/scripts/nodes.txt` that was just generated by `ec2.py` to
`ray/scripts/hosts.txt` and remove the private IP from each line (everything
after the comma, including the comma). You can then install Anaconda on the
cluster by running these commands inside of ray/scripts:

*Note*: The `parallel-ssh` is called `pssh` on Mac OS X!

```
ssh-add <path to .pem file>
parallel-ssh -x "-o StrictHostKeyChecking=no" -l ubuntu -h hosts.txt "wget http://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh"
parallel-ssh -l ubuntu -h hosts.txt "bash ~/Anaconda2-4.1.1-Linux-x86_64.sh -b"
parallel-ssh -l ubuntu -h hosts.txt 'echo -e "export PATH=\$HOME/anaconda2/bin/:\$PATH\n$(cat ~/.bashrc)" > ~/.bashrc'
```

(If an error occurs for any of these commands, you can get the error message by
adding the -i flat to parallel-ssh.)


Then inside the same directory,

```
python cluster.py --nodes=nodes.txt --key-file=awskey.pem --username=ubuntu
```

Inside the shell, run `cluster.install_ray()`.

Install further dependencies:

```
parallel-ssh -l ubuntu -h hosts.txt "sudo apt-get install -y pachi"
parallel-ssh -l ubuntu -h hosts.txt  'pip install typing funcsigs subprocess32 protobuf colorama graphviz cloudpickle theano gym pachi_py keras tabulate typing'
```

Now, check out the modular_rl source code on your local machine:

```
cd ~
git checkout https://github.com/pcmoritz/modular_rl
cd modular_rl
git checkout remote
```

In the ray cluster.py shell now execute

```
cluster.start_ray(user_source_directory="~/modular_rl", num_workers_per_node=16)
```

Now, ssh to the head node of your cluster (the first line of nodes.txt), edit
`~/user_source_files/modular_rl/run_pg.py` and replace the `ray.init` line
with the line that `cluster.start_ray` printed. Then run

```
source /home/ubuntu/ray/setup-env.sh
./run_pg.py --env Go9x9-v0 --agent modular_rl.agentzoo.TrpoAgent --video=0 \
            --remote 1 --timesteps_per_batch=2000 --hid_sizes=244,81 \
            --n_rollouts=40 --n_iter=1000
```
